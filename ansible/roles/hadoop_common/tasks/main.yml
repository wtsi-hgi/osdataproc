---

- name: Ensure Hadoop directories exist
  file:
          state: directory
          path: "{{ item }}"
          mode: 0755
          owner: "{{ hadoop_user }}"
          group: "{{ hadoop_group }}"
  loop:
          - "{{ hadoop_download_dir }}"
          - "{{ hadoop_install_dir }}"
          - "{{ hadoop_logs_dir }}"
          - "{{ hadoop_conf_dir }}"
          - "{{ hdfs_home }}"
          - "{{ hdfs_namenode_dir }}"
          - "{{ hdfs_datanode_dir }}"
          - "{{ yarn_home }}"
          - "{{ yarn_logs_dir }}"

- name: Echo Hadoop download URL
  ansible.builtin.debug:
    msg: "{{ hadoop_mirror }}/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"

- name: Use local Hadoop tarball if present
  become: yes
  copy:
          src: "{{ osdataproc.downloads_dir | expanduser }}/hadoop-{{ hadoop_version }}.tar.gz"
          dest: "{{ hadoop_download_dir }}/hadoop.tar.gz"
          mode: '0644'
  when: lookup('file', (osdataproc.downloads_dir | expanduser) ~ '/hadoop-' ~ hadoop_version ~ '.tar.gz', errors='ignore') | length > 0
  tags: master

# - name: Download Hadoop distribution from S3 cache (if available)
#   become: yes
#   get_url:
#           url: "{{ cache_base_url }}/hadoop-{{ hadoop_version }}.tar.gz"
#           dest: "{{ hadoop_download_dir }}/hadoop.tar.gz"
#           timeout: 60
#   when: lookup('file', '/home/ubuntu/work/hail-cluster-openstack/downloads/hadoop-' ~ hadoop_version ~ '.tar.gz', errors='ignore') | length == 0
#   ignore_errors: yes
#   register: hadoop_s3_get
#   tags: master

- name: Download Hadoop distribution (about 1Gb, can take a while)
  become: yes
  get_url:
          url: "{{ hadoop_mirror }}/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
          dest: "{{ hadoop_download_dir }}/hadoop.tar.gz"
          timeout: 60
  when: lookup('file', (osdataproc.downloads_dir | expanduser) ~ '/hadoop-' ~ hadoop_version ~ '.tar.gz', errors='ignore') | length == 0 and (hadoop_s3_get is not defined or hadoop_s3_get is failed)
  tags: master

- name: Extract Hadoop distribution
  become: yes
  unarchive:
          src: "{{ hadoop_download_dir }}/hadoop.tar.gz"
          dest: "{{ hadoop_install_dir }}"
          owner: "{{ hadoop_user }}"
          group: "{{ hadoop_group }}"
          extra_opts: [--strip-components=1]
          keep_newer: yes
          remote_src: yes
  tags: master

- name: Copy Hadoop configuration files
  become: yes
  template:
          src: "{{ item }}"
          dest: "{{ hadoop_home }}/etc/hadoop/{{ item | basename | regex_replace('.j2','') }}"
          owner: "{{ hadoop_user }}"
          group: "{{ hadoop_group }}"
  with_fileglob:
          - ../templates/*.j2

- name: Create Hadoop systemd service files
  become: yes
  template:
          src: "{{ item }}"
          dest: "/etc/systemd/system/{{ item | basename | regex_replace('.j2','') }}"
          owner: root
          group: root
  with_fileglob:
          - ../templates/systemd/*.j2 

- name: Ensure netdata extra HDFS directories exist
  file:
          state: directory
          path: "/etc/netdata/{{ item }}/"
          owner: root
          group: root
  loop:
          - go.d
          - health.d

- name: Create netdata hdfs.conf file
  template: src=netdata/hdfs.conf.j2 dest=/etc/netdata/go.d/hdfs.conf mode=0644
  tags: master

- name: Create netdata stream.conf file
  template: src=netdata/stream.conf.j2 dest=/etc/netdata/stream.conf mode=0644

- name: Create netdata health.hdfs.conf file
  template: src=netdata/health.hdfs.conf.j2 dest=/etc/netdata/health.d/hdfs.conf mode=0644

- name: Create netdata health_alarm_notify.conf file
  template: src=netdata/health_alarm_notify.conf.j2 dest=/etc/netdata/health_alarm_notify.conf mode=0644
